{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Astropy\n",
    "import astropy\n",
    "\n",
    "#Astropy FITS/Table handling\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.table import Table, Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "### Import Data ###\n",
    "###################\n",
    "\n",
    "# parsec\n",
    "''' Ages used 8 to 10.1 steps 0.15'''\n",
    "''' Metallicity used -2.6 to 0.1 steps 0.099'''\n",
    "\n",
    "parsecall = ascii.read('/Users/joshpovick/Desktop/Research/LMC_Ages/logisochrones.dat', \\\n",
    "                    format='basic', delimiter='\\s')\n",
    "\n",
    "rgb = np.where(parsecall['label']==3)\n",
    "parsec = parsecall[rgb]\n",
    "\n",
    "# r13\n",
    "r13 = fits.getdata('/Users/joshpovick/Desktop/Research/LMC_Ages/lmc_rgbmembers.r13-l33-58672.fits.gz')\n",
    "cln = np.where((r13['FE_H']>-9999.0)&(r13['AK_TARG']>-100.0)&(r13['LOGG']>=0.0)&\n",
    "                (r13['M_H_ERR']>-100.0)&(r13['C_FE']>-100.0)&(r13['N_FE']>-100.0))\n",
    "r13 = r13[cln]\n",
    "\n",
    "# Diane's Ages\n",
    "pdfout = fits.getdata('LMC_DR16_all_PDF.fits.gz', 1)\n",
    "xy, r13_ind, pdfout_ind = np.intersect1d(r13['APOGEE_ID'],pdfout['OBJ'],return_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='20'>\n",
    "    <b>\n",
    "        Create Neural Network\n",
    "    </b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "class Neural_Network(object):\n",
    "    '''\n",
    "    Create a 3 layered (input, hidden, output) feedforward neural network with back propagation and bias. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_width, hidden_width, learning_rate, number_of_epochs):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ----------\n",
    "            input_width: (int) number of nodes in input layer\n",
    "            hidden_width: (int) number of nodes in hidden layer\n",
    "            learning_rate: (float) learning rate \n",
    "            number_of_epochs: (int) how many epochs to use when training\n",
    "        '''\n",
    "        \n",
    "        # Widths of layers\n",
    "        self.InputWidth = input_width\n",
    "        self.HiddenWidth = hidden_width\n",
    "        self.OutputWidth = 1\n",
    "        \n",
    "        # Weights\n",
    "        self.Weights1 = np.random.randn(self.InputWidth,self.HiddenWidth)*0.2\n",
    "        self.Weights2 = np.random.randn(self.HiddenWidth,self.OutputWidth)*0.2\n",
    "        \n",
    "        # Hidden Layer biases\n",
    "        self.b1 = np.random.randn(self.HiddenWidth)\n",
    "        self.b2 = np.random.randn(self.OutputWidth)\n",
    "        \n",
    "        # learning rate\n",
    "        self.eta = learning_rate \n",
    "        \n",
    "        # number of epochs\n",
    "        self.epochs = number_of_epochs\n",
    "        \n",
    "   \n",
    "    # Sigmoid activation function and derivative\n",
    "    def Sigmoid(self,z):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ----------\n",
    "            z: (float) number to plug into sigmoid function\n",
    "        '''\n",
    "        return 1./(1.+np.exp(-np.float128(z)))\n",
    "    \n",
    "    \n",
    "    def DerivSigmoid(self,z):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ----------\n",
    "            z: (float) number to plug into derivative of sigmoid function\n",
    "            sigmoid already applied to z\n",
    "        '''\n",
    "        return self.Sigmoid(z)*(1.-self.Sigmoid(z))\n",
    "#         return z*(1.-z)\n",
    "    \n",
    "    \n",
    "    # Propagation functions\n",
    "    def FeedForward(self,X):\n",
    "        '''\n",
    "        Parameters: \n",
    "        ----------\n",
    "            X: dataset of inputs\n",
    "        '''\n",
    "        # feed through whole neural network\n",
    "        self.First = np.dot(X, self.Weights1) + self.b1\n",
    "        self.Second = self.Sigmoid(self.First)\n",
    "        Third = np.dot(self.Second,self.Weights2) #+ self.b2\n",
    "        \n",
    "        return Third\n",
    "        \n",
    "    \n",
    "    def BackPropagation(self,X,yPredicted,yObserved): \n",
    "        '''\n",
    "        Parameters:\n",
    "        ----------\n",
    "            X: dataset of inputs\n",
    "            yPredicted: known output for X\n",
    "            yObserved: calculated outputs going from input layer to output layer once\n",
    "        '''\n",
    "        # Output layer error and output delta\n",
    "        self.OutputError = yPredicted - yObserved\n",
    "        self.OutputDelta = self.OutputError*self.DerivSigmoid(yObserved)\n",
    "        \n",
    "        # Hidden layer error and hidden delta\n",
    "        self.HiddenError = np.dot(self.OutputDelta,self.Weights2.T)\n",
    "        self.HiddenDelta = self.HiddenError*self.DerivSigmoid(self.First)\n",
    "        #self.HiddenDelta = self.HiddenError*self.DerivSigmoid(self.Second)\n",
    "        \n",
    "        # Update weights \n",
    "        self.Weights1 += self.eta*np.dot(X.T,self.HiddenDelta)\n",
    "        self.Weights2 += self.eta*np.dot(self.Second.T,self.OutputDelta)\n",
    "        \n",
    "#         print(self.b)\n",
    "#         print(np.ones(self.HiddenWidth))\n",
    "#         print(self.HiddenDelta)\n",
    "\n",
    "#         print('bias')\n",
    "#         print(self.b)\n",
    "#         print('bias adjust')\n",
    "#         print(self.HiddenWidth)\n",
    "#         print(self.HiddenDelta)\n",
    "#         print(np.shape(self.HiddenDelta))\n",
    "#         print(self.eta*np.dot(np.ones(self.HiddenWidth).T,self.HiddenDelta))\n",
    "#         print(np.add(self.b,self.eta*np.dot(np.ones(self.HiddenWidth),self.HiddenDelta)))\n",
    "        \n",
    "        # Updates biases\n",
    "        self.b1 += self.eta*np.dot(np.ones(len(self.First)),self.HiddenDelta)\n",
    "#         self.b2 += self.eta*np.dot(np.ones(len(self.FeedForward(X))),self.OutputDelta)\n",
    "#         self.b = self.b + self.eta*np.dot(np.ones(self.HiddenWidth),self.HiddenDelta)\n",
    "#         self.b = np.add(self.b,self.eta*np.dot(np.ones(self.HiddenWidth),self.HiddenDelta))\n",
    "    \n",
    "   \n",
    "    # Train on dataset\n",
    "    def Train(self,X,yPredicted):\n",
    "        '''\n",
    "        Train neural network on data in epochs and print final loss\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "            X: dataset of inputs to train\n",
    "            yPredicted: known output for X\n",
    "        '''\n",
    "        \n",
    "        # feedforward and backpropagate for the specified number of epochs\n",
    "        for i in tqdm_notebook(range(self.epochs)):\n",
    "            self.BackPropagation(X,yPredicted,self.FeedForward(X))\n",
    "        \n",
    "        # print out loss\n",
    "        print('Training Loss (MSE): ', np.mean(np.square(yPredicted - self.FeedForward(X))))\n",
    "        \n",
    "    \n",
    "    # Predict output from new input\n",
    "    def Predict(self,XNew):\n",
    "        '''\n",
    "        Predict the output given a new dataset XNew\n",
    "        '''\n",
    "        return self.FeedForward(XNew)\n",
    "    \n",
    "    \n",
    "    # Utility functions\n",
    "    def SaveWeights(self):\n",
    "        '''\n",
    "        Save weights from the input layer to the hidden layer as 'Weights1.txt'\n",
    "        and save weights from hidden layer to output as 'Weights2.txt'.\n",
    "        '''\n",
    "        np.savetxt('Weights1.txt',self.Weights1,fmt='%s')\n",
    "        np.savetxt('Weights2.txt',self.Weights2,fmt='%s')\n",
    "        np.savetxt('b.txt',self.b1,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "# X = (hours studying, hours sleeping), y = score on test\n",
    "xAll = np.array([parsec['logTe'],parsec['Ksmag'],np.log10(parsec['Zini']/0.02),parsec['logg']]).T #np.array(([2, 9], [1, 5], [3, 6], [5, 10]), dtype=float) # input data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(xAll)\n",
    "scxAll = scaler.transform(xAll)\n",
    "\n",
    "y = parsec['logAge']/10. #np.array(([92], [86], [89]), dtype=float) # output\n",
    "\n",
    "# scale units\n",
    "# xAll = xAll/np.amax(xAll, axis=0) # scaling input data\n",
    "# y = y/100 # scaling output data (max test score is 100)\n",
    "\n",
    "# # split data\n",
    "# X = np.split(xAll, [3])[0] # training data\n",
    "# XNew = np.split(xAll, [3])[1] # testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scxAll, y, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([parsec['logTe'],parsec['Ksmag'],np.log10(parsec['Zini']/0.02),parsec['logg'],\n",
    "#           np.multiply(parsec['logTe'],parsec['Ksmag']),np.multiply(parsec['logTe'],np.log10(parsec['Zini']/0.02)),\n",
    "#           np.multiply(parsec['logTe'],parsec['logg']),np.multiply(parsec['Ksmag'],np.log10(parsec['Zini']/0.02)),\n",
    "#           np.multiply(parsec['Ksmag'],parsec['logg']),np.multiply(np.log10(parsec['Zini']/0.02),parsec['logg']),\n",
    "#           np.square(parsec['logTe']),np.square(parsec['Ksmag']),np.square(np.log10(parsec['Zini']/0.02)),\n",
    "#           np.square(parsec['logg'])])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:128: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea5d4e3304a4df1973b59c7e27c448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss (MSE):  logAge    9.561139e+06\n",
      "dtype: float128\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network(input_width=4,hidden_width=14,learning_rate=0.5,number_of_epochs=150000)\n",
    "NN.Train(pd.DataFrame(X_train),pd.DataFrame(y_train))\n",
    "NN.SaveWeights()\n",
    "# NN.Predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.472e+03, 6.260e+02, 3.130e+02, 1.320e+02, 7.100e+01, 3.300e+01,\n",
       "        1.400e+01, 3.000e+00, 4.000e+00, 5.000e+00]),\n",
       " array([3093.04647739, 3093.05491651, 3093.06335563, 3093.07179475,\n",
       "        3093.08023387, 3093.08867299, 3093.0971121 , 3093.10555122,\n",
       "        3093.11399034, 3093.12242946, 3093.13086858], dtype=float128),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(NN.Predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10771</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10772</th>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10773</th>\n",
       "      <td>9.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10774</th>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10775</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10776</th>\n",
       "      <td>9.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10777</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10778</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779</th>\n",
       "      <td>9.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10780</th>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10781</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10782</th>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>9.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>9.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10790</th>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10792</th>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10793</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10795</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797</th>\n",
       "      <td>8.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10798</th>\n",
       "      <td>9.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10801 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       logAge\n",
       "0        9.50\n",
       "1        9.35\n",
       "2        8.60\n",
       "3        9.50\n",
       "4        9.50\n",
       "5        9.50\n",
       "6        8.45\n",
       "7        8.45\n",
       "8        9.20\n",
       "9        9.50\n",
       "10       9.65\n",
       "11       9.35\n",
       "12       8.45\n",
       "13       8.90\n",
       "14       8.00\n",
       "15       9.80\n",
       "16       9.80\n",
       "17       9.35\n",
       "18       9.05\n",
       "19       9.35\n",
       "20       9.20\n",
       "21       9.95\n",
       "22       9.50\n",
       "23       9.50\n",
       "24       9.80\n",
       "25       9.05\n",
       "26       9.80\n",
       "27       8.45\n",
       "28       9.80\n",
       "29       8.90\n",
       "...       ...\n",
       "10771    9.50\n",
       "10772   10.10\n",
       "10773    9.65\n",
       "10774    8.45\n",
       "10775    9.50\n",
       "10776    9.65\n",
       "10777    9.50\n",
       "10778    9.50\n",
       "10779    9.20\n",
       "10780    8.90\n",
       "10781    9.80\n",
       "10782    9.95\n",
       "10783    9.35\n",
       "10784    9.95\n",
       "10785    9.05\n",
       "10786    9.50\n",
       "10787    9.50\n",
       "10788    9.35\n",
       "10789    8.90\n",
       "10790    9.95\n",
       "10791    9.95\n",
       "10792    9.95\n",
       "10793    9.80\n",
       "10794    9.80\n",
       "10795    9.80\n",
       "10796    8.00\n",
       "10797    8.75\n",
       "10798    9.65\n",
       "10799    9.50\n",
       "10800    9.80\n",
       "\n",
       "[10801 rows x 1 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)\n",
    "pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
